{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import tempfile\n",
    "import chassisml\n",
    "import numpy as np\n",
    "import getpass\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from shutil import rmtree\n",
    "import json\n",
    "import onnx\n",
    "from onnx import backend\n",
    "from onnx import numpy_helper\n",
    "import onnxruntime as ort\n",
    "from transformers import GPT2Model, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter credentials\n",
    "Dockerhub creds and Modzy API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker hub username········\n",
      "docker hub password········\n",
      "modzy api key········\n"
     ]
    }
   ],
   "source": [
    "dockerhub_user = getpass.getpass('docker hub username')\n",
    "dockerhub_pass = getpass.getpass('docker hub password')\n",
    "modzy_api_key = getpass.getpass('modzy api key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ONNX Model and Test Locally\n",
    "This model was downloaded from the [ONNX Model Zoo](https://github.com/onnx/models/tree/master/text/machine_comprehension/gpt-2), which contains several pre-trained models saved in the ONNX open standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gpt-2 and gpt-2 head models are valid onnx models files\n",
    "model = onnx.load(\"models/model.onnx\")\n",
    "head_model = onnx.load(\"models/head_model.onnx\")\n",
    "\n",
    "# check onnx file is valid model\n",
    "onnx.checker.check_model(model)\n",
    "onnx.checker.check_model(head_model)\n",
    "\n",
    "# load input tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING FUNTIONS NEEDED\n",
    "def flatten(inputs):\n",
    "    return [[flatten(i) for i in inputs] if isinstance(inputs, (list, tuple)) else inputs]\n",
    "def to_numpy(x):\n",
    "    if type(x) is not np.ndarray:\n",
    "        x = x.detach().cpu().numpy().astype(np.int64) if x.requires_grad else x.cpu().numpy().astype(np.int64)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "length = 10\n",
    "\n",
    "text = \"this is a test\"\n",
    "tokens = np.array(tokenizer.encode(text, add_special_tokens=True))\n",
    "tensors = torch.tensor([[tokens]])\n",
    "\n",
    "prev = tensors\n",
    "output = tensors\n",
    "\n",
    "for i in range(length):\n",
    "    session = ort.InferenceSession(\"models/head_model.onnx\")\n",
    "    ort_inputs = dict((session.get_inputs()[i].name, to_numpy(input)) for i, input in enumerate(flatten(prev)))\n",
    "    outputs = session.run(None, ort_inputs)\n",
    "    logits = torch.from_numpy(outputs[0])\n",
    "    logits = logits[:, -1, :]\n",
    "    log_probs = F.softmax(logits, dim=-1)\n",
    "    _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "#     print(output.shape, prev.shape)\n",
    "    prev = torch.reshape(prev, (1, prev.shape[2], prev.shape[1]))\n",
    "#     print(output.shape, prev.shape)\n",
    "#     sdfadsf\n",
    "    output = torch.cat((output, prev), dim=1)\n",
    "#     print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[:, len(tokens):].tolist()\n",
    "generated = 0\n",
    "for i in range(batch_size):\n",
    "    generated += 1\n",
    "    text = tokenizer.decode(output[i])\n",
    "    print(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "length = 10\n",
    "\n",
    "text = \"Here is some text to encode : Hello World!\"\n",
    "tokens = np.array(tokenizer.encode(text))\n",
    "context = torch.tensor(tokens, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "prev = context\n",
    "output = context\n",
    "\n",
    "for i in range(length):\n",
    "    outputs = model(prev)\n",
    "    logits = outputs[0]\n",
    "    logits = logits[:, -1, :]\n",
    "    log_probs = F.softmax(logits, dim=-1)\n",
    "    _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "    output = torch.cat((output, prev), dim=1)\n",
    "\n",
    "output = output[:, len(tokens):].tolist()\n",
    "generated = 0\n",
    "for i in range(batch_size):\n",
    "    generated += 1\n",
    "    text = tokenizer.decode(output[i])\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[:, len(tokens):].tolist()\n",
    "generated = 0\n",
    "for i in range(batch_size):\n",
    "    generated += 1\n",
    "    text = tokenizer.decode(output[i])\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is a test\"\n",
    "tokens = np.array(tokenizer.encode(text, add_special_tokens=True))\n",
    "tensors = torch.tensor([[tokens]])\n",
    "\n",
    "prev = tensors\n",
    "output = tensors\n",
    "\n",
    "for i in range(length):\n",
    "    session = ort.InferenceSession(\"models/head_model.onnx\")\n",
    "    ort_inputs = dict((session.get_inputs()[i].name, to_numpy(input)) for i, input in enumerate(flatten(prev)))\n",
    "    outputs = session.run(None, ort_inputs)\n",
    "    logits = torch.from_numpy(outputs[0])\n",
    "    logits = logits[:, -1, :]\n",
    "    log_probs = F.softmax(logits, dim=-1)\n",
    "    _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "#     print(output.shape, prev.shape)\n",
    "    prev = torch.reshape(prev, (1, prev.shape[2], prev.shape[1]))\n",
    "    output = torch.cat((output, prev), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BRADLE~1\\AppData\\Local\\Temp/ipykernel_24692/4074800289.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mgenerated\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\chassis-demo\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3292\u001b[1;33m         return self._decode(\n\u001b[0m\u001b[0;32m   3293\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\chassis-demo\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decode_use_source_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"use_source_tokenizer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m         \u001b[0mfiltered_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[1;31m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\chassis-demo\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "output = output[:, len(tokens):].tolist()\n",
    "generated = 0\n",
    "for i in range(batch_size):\n",
    "    generated += 1\n",
    "    text = tokenizer.decode(output[i])\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BRADLE~1\\AppData\\Local\\Temp/ipykernel_24692/2298069106.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mgenerated\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     print(output[i])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\chassis-demo\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3292\u001b[1;33m         return self._decode(\n\u001b[0m\u001b[0;32m   3293\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\chassis-demo\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decode_use_source_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"use_source_tokenizer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m         \u001b[0mfiltered_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[1;31m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\envs\\chassis-demo\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "generated = 0\n",
    "for i in range(batch_size):\n",
    "    generated += 1\n",
    "#     print(output[i])\n",
    "    text = tokenizer.decode(output[i])\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4])\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "print(tensors.shape) \n",
    "print(tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'this is a test'\n",
    "tokens = np.array(gpt2_tokenizer.encode(text))\n",
    "tensors = torch.tensor(tokens, device=\"cpu\", dtype=torch.long).unsqueeze(0)#.repeat(batch_size, 1)\n",
    "tensors = torch.reshape(tensors, (1, tensors.shape[0], tensors.shape[1]))\n",
    "\n",
    "session = ort.InferenceSession(\"models/head_model.onnx\")\n",
    "ort_inputs = dict((session.get_inputs()[i].name, to_numpy(input)) for i, input in enumerate(flatten(tensors)))\n",
    "outputs = session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying post processing\n",
    "batch_size = 1\n",
    "length = 10\n",
    "text = 'ONNX Models are great. Chassisml makes it so easy to deploy it!'\n",
    "tokens = np.array(gpt2_tokenizer.encode(text))\n",
    "tensors = torch.tensor(tokens, device=\"cpu\", dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "tensors = torch.reshape(tensors, (1, tensors.shape[0], tensors.shape[1]))\n",
    "prev = tensors\n",
    "output = tensors\n",
    "\n",
    "# tensors = torch.tensor([[tokens]])\n",
    "# tensors = torch.tensor([\n",
    "#     [tokenizer.encode(text, add_special_tokens=True)]\n",
    "# ])\n",
    "\n",
    "# prev = tensors\n",
    "# # output = torch.reshape(tensors, (1, tensors.shape[2], tensors.shape[1]))\n",
    "# output = tensors\n",
    "print(tokens.shape, tensors.shape)\n",
    "\n",
    "for i in range(length):\n",
    "    session = ort.InferenceSession(\"models/head_model.onnx\")\n",
    "    ort_inputs = dict((session.get_inputs()[i].name, to_numpy(input)) for i, input in enumerate(flatten(prev)))\n",
    "    outputs = session.run(None, ort_inputs)\n",
    "    logits = torch.from_numpy(outputs[0])\n",
    "    logits = logits[:, -1, :]\n",
    "    log_probs = F.softmax(logits, dim=-1)\n",
    "    _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "    print(output.shape, prev.shape)\n",
    "    prev = torch.reshape(prev, (1, prev.shape[2], prev.shape[1]))\n",
    "    output = torch.cat((output, prev), dim=1)\n",
    "\n",
    "print(output, output.shape)\n",
    "output = output[:, len(tokens):].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,) torch.Size([1, 1, 19])\n",
      "torch.Size([1, 1, 19]) torch.Size([1, 19, 1])\n",
      "torch.Size([1, 2, 19]) torch.Size([1, 19, 1])\n",
      "torch.Size([1, 3, 19]) torch.Size([1, 19, 1])\n",
      "torch.Size([1, 4, 19]) torch.Size([1, 19, 1])\n",
      "torch.Size([1, 5, 19]) torch.Size([1, 19, 1])\n",
      "torch.Size([1, 6, 19]) torch.Size([1, 19, 1])\n",
      "torch.Size([1, 7, 19]) torch.Size([1, 19, 1])\n",
      "torch.Size([1, 8, 19]) torch.Size([1, 19, 1])\n",
      "torch.Size([1, 9, 19]) torch.Size([1, 19, 1])\n",
      "torch.Size([1, 10, 19]) torch.Size([1, 19, 1])\n",
      "tensor([[[ 1340,    45,    55, 32329,   389,  1049,    13,   609,   562,  1042,\n",
      "             75,  1838,   340,   523,  2562,   284,  6061,   340,     0],\n",
      "         [   13,  4877,    12,   198,  1695,   329,   314,  1817, 21080,   318,\n",
      "            318,   606,  2562,   345,   284,   651,   290,    13,   198],\n",
      "         [  198,    13,    16,   198,   287,  4321,  2640,    11,    11,   257,\n",
      "            257,  1512,   284,   460,   779,   340,   484,   198,   198],\n",
      "         [  198,   198,    13,   198,   262,  4235,    13,   198,   198,   649,\n",
      "             13,    11,   262,   307,    11,    11,   460,   198,    13],\n",
      "         [  198,   464,   198,   198,   717,   286,   198,   198,    13,  4235,\n",
      "            198,   198,  4235, 17396,   262,   262,    11,   198,   198],\n",
      "         [  198,   717,   198,     1,   198,   262,   198,  1169,   198,   198,\n",
      "            198,    13,   198,   198,   198,   198,   198,   198,    13],\n",
      "         [  198,    62,   198,    40,   198,   717,     1,   198,   198,     1,\n",
      "              1,   198,   198,     1,     1,     1,     1,     1,   198],\n",
      "         [  198,   198,   198,  1101,   198,   761,   198,   198,    62,    40,\n",
      "            198,   198,     1,     1,   198,   198,   198,     1,   198],\n",
      "         [  198,   464,   464,    64,   198,   198,   198,    40,   198,   198,\n",
      "            198,    62,   198,   198,   198,     1,     1,   198,   198],\n",
      "         [  198,   717,  1708,   338,   198,   464,   464,  1101,   198,    40,\n",
      "             40,   198,   198,    62,    62,   198,   198,   198,     1],\n",
      "         [  198,    62,   262,   257,   198,   717,   717,    64,   198,  1101,\n",
      "            198,   198,    40,   198,   198,   198,    62,    62,   198]]]) torch.Size([1, 11, 19])\n"
     ]
    }
   ],
   "source": [
    "# trying post processing - not working correctly \n",
    "batch_size = 1\n",
    "length = 10\n",
    "text = 'ONNX Models are great. Chassisml makes it so easy to deploy it!'\n",
    "tokens = np.array(gpt2_tokenizer.encode(text))\n",
    "tensors = torch.tensor(tokens, device=\"cpu\", dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n",
    "tensors = torch.reshape(tensors, (1, tensors.shape[0], tensors.shape[1]))\n",
    "prev = tensors\n",
    "output = tensors\n",
    "\n",
    "# tensors = torch.tensor([[tokens]])\n",
    "# tensors = torch.tensor([\n",
    "#     [tokenizer.encode(text, add_special_tokens=True)]\n",
    "# ])\n",
    "\n",
    "# prev = tensors\n",
    "# # output = torch.reshape(tensors, (1, tensors.shape[2], tensors.shape[1]))\n",
    "# output = tensors\n",
    "print(tokens.shape, tensors.shape)\n",
    "\n",
    "for i in range(length):\n",
    "    session = ort.InferenceSession(\"models/head_model.onnx\")\n",
    "    ort_inputs = dict((session.get_inputs()[i].name, to_numpy(input)) for i, input in enumerate(flatten(prev)))\n",
    "    outputs = session.run(None, ort_inputs)\n",
    "    logits = torch.from_numpy(outputs[0])\n",
    "    logits = logits[:, -1, :]\n",
    "    log_probs = F.softmax(logits, dim=-1)\n",
    "    _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "    print(output.shape, prev.shape)\n",
    "    prev = torch.reshape(prev, (1, prev.shape[2], prev.shape[1]))\n",
    "    output = torch.cat((output, prev), dim=1)\n",
    "\n",
    "print(output, output.shape)\n",
    "output = output[:, len(tokens):].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "output_text = []\n",
    "batch_size=1\n",
    "generated = 0\n",
    "for i in range(batch_size):\n",
    "    generated += 1\n",
    "    print(output[i])\n",
    "    text = tokenizer.decode(output[i])\n",
    "    output_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[13],\n",
       "  [4877],\n",
       "  [12],\n",
       "  [198],\n",
       "  [1695],\n",
       "  [329],\n",
       "  [314],\n",
       "  [1817],\n",
       "  [21080],\n",
       "  [318],\n",
       "  [318],\n",
       "  [606],\n",
       "  [2562],\n",
       "  [345],\n",
       "  [284],\n",
       "  [651],\n",
       "  [290],\n",
       "  [13],\n",
       "  [198],\n",
       "  [13],\n",
       "  [4877],\n",
       "  [12],\n",
       "  [198],\n",
       "  [1695],\n",
       "  [329],\n",
       "  [314],\n",
       "  [1817],\n",
       "  [21080],\n",
       "  [318],\n",
       "  [318],\n",
       "  [606],\n",
       "  [2562],\n",
       "  [345],\n",
       "  [284],\n",
       "  [651],\n",
       "  [290],\n",
       "  [13],\n",
       "  [198],\n",
       "  [13],\n",
       "  [4877],\n",
       "  [12],\n",
       "  [198],\n",
       "  [1695],\n",
       "  [329],\n",
       "  [314],\n",
       "  [1817],\n",
       "  [21080],\n",
       "  [318],\n",
       "  [318],\n",
       "  [606],\n",
       "  [2562],\n",
       "  [345],\n",
       "  [284],\n",
       "  [651],\n",
       "  [290],\n",
       "  [13],\n",
       "  [198],\n",
       "  [13],\n",
       "  [4877],\n",
       "  [12],\n",
       "  [198],\n",
       "  [1695],\n",
       "  [329],\n",
       "  [314],\n",
       "  [1817],\n",
       "  [21080],\n",
       "  [318],\n",
       "  [318],\n",
       "  [606],\n",
       "  [2562],\n",
       "  [345],\n",
       "  [284],\n",
       "  [651],\n",
       "  [290],\n",
       "  [13],\n",
       "  [198],\n",
       "  [13],\n",
       "  [4877],\n",
       "  [12],\n",
       "  [198],\n",
       "  [1695],\n",
       "  [329],\n",
       "  [314],\n",
       "  [1817],\n",
       "  [21080],\n",
       "  [318],\n",
       "  [318],\n",
       "  [606],\n",
       "  [2562],\n",
       "  [345],\n",
       "  [284],\n",
       "  [651],\n",
       "  [290],\n",
       "  [13],\n",
       "  [198],\n",
       "  [13],\n",
       "  [4877],\n",
       "  [12],\n",
       "  [198],\n",
       "  [1695],\n",
       "  [329],\n",
       "  [314],\n",
       "  [1817],\n",
       "  [21080],\n",
       "  [318],\n",
       "  [318],\n",
       "  [606],\n",
       "  [2562],\n",
       "  [345],\n",
       "  [284],\n",
       "  [651],\n",
       "  [290],\n",
       "  [13],\n",
       "  [198],\n",
       "  [13],\n",
       "  [4877],\n",
       "  [12],\n",
       "  [198],\n",
       "  [1695],\n",
       "  [329],\n",
       "  [314],\n",
       "  [1817],\n",
       "  [21080],\n",
       "  [318],\n",
       "  [318],\n",
       "  [606],\n",
       "  [2562],\n",
       "  [345],\n",
       "  [284],\n",
       "  [651],\n",
       "  [290],\n",
       "  [13],\n",
       "  [198],\n",
       "  [13],\n",
       "  [4877],\n",
       "  [12],\n",
       "  [198],\n",
       "  [1695],\n",
       "  [329],\n",
       "  [314],\n",
       "  [1817],\n",
       "  [21080],\n",
       "  [318],\n",
       "  [318],\n",
       "  [606],\n",
       "  [2562],\n",
       "  [345],\n",
       "  [284],\n",
       "  [651],\n",
       "  [290],\n",
       "  [13],\n",
       "  [198],\n",
       "  [13],\n",
       "  [4877],\n",
       "  [12],\n",
       "  [198],\n",
       "  [1695],\n",
       "  [329],\n",
       "  [314],\n",
       "  [1817],\n",
       "  [21080],\n",
       "  [318],\n",
       "  [318],\n",
       "  [606],\n",
       "  [2562],\n",
       "  [345],\n",
       "  [284],\n",
       "  [651],\n",
       "  [290],\n",
       "  [13],\n",
       "  [198],\n",
       "  [13],\n",
       "  [4877],\n",
       "  [12],\n",
       "  [198],\n",
       "  [1695],\n",
       "  [329],\n",
       "  [314],\n",
       "  [1817],\n",
       "  [21080],\n",
       "  [318],\n",
       "  [318],\n",
       "  [606],\n",
       "  [2562],\n",
       "  [345],\n",
       "  [284],\n",
       "  [651],\n",
       "  [290],\n",
       "  [13],\n",
       "  [198]]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be passed to Chassis:\n",
    "context = {\n",
    "    \"model\": head_model,\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"utilities\": {\n",
    "        \"flatten\": flatten,\n",
    "        \"to_numpy\": to_numpy\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write process function\n",
    "\n",
    "* Must take bytes and context dict as input\n",
    "* Preprocess bytes, run inference, postprocess model output, return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(input_bytes,context):\n",
    "    length = 10\n",
    "    # save model to filepath for inference\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    import onnx\n",
    "    onnx.save(context[\"model\"], \"{}/model.onnx\".format(tmp_dir))\n",
    "    \n",
    "    text = input_bytes.decode()\n",
    "    tokens = np.array(context[\"tokenizer\"].encode(text, add_special_tokens=False))\n",
    "    tokens = np.array(gpt2_tokenizer.encode(text, add_special_tokens=False))\n",
    "    tensors = torch.tensor([[tokens]])\n",
    "\n",
    "    prev = context\n",
    "    output = context\n",
    "\n",
    "    for i in range(length):\n",
    "        session = ort.InferenceSession(\"{}/model.onnx\".format(tmp_dir))\n",
    "        ort_inputs = dict((session.get_inputs()[i].name, context[\"utilities\"][\"to_numpy\"](input)) for i, input in enumerate(context[\"flatten\"](tensor_inputs)))\n",
    "        outputs = session.run(None, ort_inputs)\n",
    "        logits = torch.from_numpy(outputs[0])\n",
    "        logits = logits[:, -1, :]\n",
    "        log_probs = F.softmax(logits, dim=-1)\n",
    "        _, prev = torch.topk(log_probs, k=1, dim=-1)\n",
    "        prev = torch.reshape(prev, (prev.shape[0], prev.shape[1]))\n",
    "        output = torch.cat((output, prev), dim=1)\n",
    "\n",
    "    output = output[:, len(tokens):].tolist()\n",
    "    generated = 0\n",
    "    output_text = []\n",
    "    for i in range(1):\n",
    "        generated += 1\n",
    "        text = tokenizer.decode(output[i])\n",
    "        output_text.append(text)\n",
    "    \n",
    "    # format results\n",
    "    structured_result = {\n",
    "        \"data\": {\n",
    "            \"result\": {\"nextWordPredictions\": [{\"word_{}\".format(i): text_pred} for i, text_pred in enumerate(output_text)]}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # remove temp directory\n",
    "    rmtree(tmp_dir)\n",
    "    return structured_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Chassis Client\n",
    "We'll use this to interact with the Chassis service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chassis_client = chassisml.ChassisClient(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and test Chassis model\n",
    "* Requires `context` dict containing all variables which should be loaded once and persist across inferences\n",
    "* Requires `process_fn` defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BRADLE~1\\AppData\\Local\\Temp/ipykernel_24692/1260425798.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msample_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data/sample_text.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchassis_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\chassisml\\chassisml.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, test_input)\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid input. Must be buffered reader, bytes, valid filepath, or text input.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\chassisml\\chassisml.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(_, model_input)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprocess_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchassis_base_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseparators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNumpyEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BRADLE~1\\AppData\\Local\\Temp/ipykernel_24692/1997753185.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(input_bytes, context)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mort\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/model.onnx\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mort_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"utilities\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"to_numpy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"flatten\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mort_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# create Chassis model\n",
    "chassis_model = chassis_client.create_model(context=context,process_fn=process)\n",
    "\n",
    "# test Chassis model locally (can pass filepath, bufferedreader, bytes, or text here):\n",
    "sample_filepath = 'data/sample_text.txt'\n",
    "results = chassis_model.test(sample_filepath)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test environment and model within Chassis service, must pass filepath here:\n",
    "\n",
    "# dry run before build\n",
    "test_env_result = chassis_model.test_env(sample_filepath)\n",
    "print(test_env_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish model to Modzy\n",
    "Need to provide model name, model version, Dockerhub credentials, and required Modzy info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chassis_model.publish(\n",
    "    model_name=\"ONNX MobileNet Image Classification\",\n",
    "    model_version=\"0.0.1\",\n",
    "    registry_user=dockerhub_user,\n",
    "    registry_pass=dockerhub_pass,\n",
    "    modzy_sample_input_path=sample_filepath,\n",
    "    modzy_api_key=modzy_api_key\n",
    ")\n",
    "\n",
    "job_id = response.get('job_id')\n",
    "final_status = chassis_client.block_until_complete(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chassis_client.get_job_status(job_id)[\"result\"] is not None:\n",
    "    print(\"New model URL: {}\".format(chassis_client.get_job_status(job_id)[\"result\"][\"container_url\"]))\n",
    "else:\n",
    "    print(\"Chassis job failed \\n\\n {}\".format(chassis_client.get_job_status(job_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sample job using Modzy SDK\n",
    "Submit inference job to our newly-deploy model running on Modzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modzy import ApiClient\n",
    "\n",
    "client = ApiClient(base_url='https://integration.modzy.engineering/api', api_key=modzy_api_key)\n",
    "\n",
    "input_name = final_status['result']['inputs'][0]['name']\n",
    "model_id = final_status['result'].get(\"model\").get(\"modelId\")\n",
    "model_version = final_status['result'].get(\"version\")\n",
    "\n",
    "inference_job = client.jobs.submit_file(model_id, model_version, {input_name: sample_filepath})\n",
    "inference_job_result = client.results.block_until_complete(inference_job, timeout=None)\n",
    "inference_job_results_json = inference_job_result.get_first_outputs()['results.json']\n",
    "print(inference_job_results_json)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95ab8d3bdd708a5b71676285742277315a30b0df1ed91c8905076616709c59d5"
  },
  "kernelspec": {
   "display_name": "chassis-demo-1",
   "language": "python",
   "name": "chassis-demo-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
