{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chassis.ml demo\n",
    "\n",
    "## Easily build MLflow models into {KFServing, Modzy} Docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo will show you how we can train a model, define custom pre- and post-processing steps, save it in MLflow format and then build it into a container image and push it to docker hub with a single command.\n",
    "\n",
    "By easily connecting MLflow models to Docker images with a simple Python SDK for data scientists & ML engineers, Chassis is the missing link between MLflow and DevOps.\n",
    "\n",
    "This demo can be run in local using minikube and a local installation of Chassis.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* [Docker Hub](https://hub.docker.com/) account (free one is fine)\n",
    "* The browser you're reading this in :-)\n",
    "* Existing local installation of Chassis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chassisml\n",
    "import mlflow.pyfunc\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model\n",
    "\n",
    "Load pretrained PyTorch ResNet50 model and ImageNet labels.\n",
    "\n",
    "The goal for Chassis service is to create an image that exposes this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "labels = pickle.load(open('./modzy/imagenet_labels.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Wrap your model in a pyfunc and provide auxiliary functionality through extension of the\n",
    "# mlflow PythonModel class with methods pre_process, post_process, and explain\n",
    "\n",
    "# Must inherit from mlflow.pyfunc.PythonModel\n",
    "class CustomModel(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        # This method is REQUIRED\n",
    "        # Load anything that needs to persist across inference runs here\n",
    "        \"\"\"\n",
    "\n",
    "        import torch\n",
    "        from torchvision import transforms\n",
    "        \n",
    "        # Note that the model and labels were loaded outside of this class in the previous cell\n",
    "        # They were simply assigned to instance variables here\n",
    "        # This is allowed\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.labels = labels\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])        \n",
    "\n",
    "    def predict(self, context, input_dict): \n",
    "        \"\"\"\n",
    "        This method is REQUIRED.\n",
    "        When an inference job comes in, this will be executed.\n",
    "        input_dict['input_data_bytes'] will contain the input file in bytes format.\n",
    "        This method must those bytes, running inference, postprocess if needed, and return results.\n",
    "        In this example, we have broken out preprocessing and postproceessing into their own methods.\n",
    "        However, if you'd like, you can handle everything within this predict() method.\n",
    "        \"\"\"\n",
    "        preprocessed_input = self.preprocess(input_dict['input_data_bytes'])\n",
    "        inference_results = self.model(preprocessed_input)\n",
    "        return self.postprocess(inference_results)\n",
    "\n",
    "    def preprocess(self, img_bytes):\n",
    "        import cv2\n",
    "        import torch\n",
    "        import numpy as np\n",
    "\n",
    "        # convert input bytes into image NumPy array \n",
    "        decoded = cv2.imdecode(np.frombuffer(img_bytes, np.uint8), -1)\n",
    "        img_t = self.transform(decoded)\n",
    "        batch_t = torch.unsqueeze(img_t, 0).to(self.device)\n",
    "        return batch_t\n",
    "    \n",
    "    def postprocess(self, predictions):\n",
    "        # postprocess model output into desired output format\n",
    "\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        \n",
    "        percentage = torch.nn.functional.softmax(predictions, dim=1)[0]\n",
    "\n",
    "        _, indices = torch.sort(predictions, descending=True)\n",
    "        inference_result = {\n",
    "            \"classPredictions\": [\n",
    "                {\"class\": self.labels[idx.item()], \"score\": percentage[idx].item()}\n",
    "            for idx in indices[0][:5] ]\n",
    "        }\n",
    "                \n",
    "        structured_output = {\n",
    "            \"data\": {\n",
    "                \"result\": inference_result,\n",
    "                \"explanation\": None,\n",
    "                \"drift\": None,\n",
    "            }\n",
    "        }\n",
    "        return structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define conda environment with all required dependencies for your model\n",
    "# mlflow is ALWAYS REQUIRED\n",
    "\n",
    "conda_env = {\n",
    "    \"channels\": [\"defaults\", \"conda-forge\", \"pytorch\"],\n",
    "    \"dependencies\": [\n",
    "        \"python=3.8.5\",\n",
    "        \"pytorch\",\n",
    "        \"torchvision\",\n",
    "        \"pip\",\n",
    "        {\n",
    "            \"pip\": [\n",
    "                \"mlflow\",\n",
    "                \"opencv-python-headless\"\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \"name\": \"torch_env\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "Transform the model into MLFlow format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mlflow_custom_pyfunc_torch\n",
    "model_save_path = \"mlflow_custom_pyfunc_torch\"\n",
    "mlflow.pyfunc.save_model(path=model_save_path, python_model=CustomModel(), conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MLFlow model and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": {\n",
      "        \"result\": {\n",
      "            \"classPredictions\": [\n",
      "                {\n",
      "                    \"class\": \"airliner\",\n",
      "                    \"score\": 0.919350802898407\n",
      "                },\n",
      "                {\n",
      "                    \"class\": \"wing\",\n",
      "                    \"score\": 0.05414900183677673\n",
      "                },\n",
      "                {\n",
      "                    \"class\": \"warplane, military plane\",\n",
      "                    \"score\": 0.010522871278226376\n",
      "                },\n",
      "                {\n",
      "                    \"class\": \"aircraft carrier, carrier, flattop, attack aircraft carrier\",\n",
      "                    \"score\": 0.004400244448333979\n",
      "                },\n",
      "                {\n",
      "                    \"class\": \"crane\",\n",
      "                    \"score\": 0.001940063084475696\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"explanation\": null,\n",
      "        \"drift\": null\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# The model's predict() method is expecting a dict where:\n",
    "# input_dict['input_data_bytes'] will contain the input file in bytes format\n",
    "# Let's mimic that here to test our saved model's functionality with an example image\n",
    "input_dict = {'input_data_bytes': open('./modzy/airplane.jpg','rb').read()}\n",
    "\n",
    "classifier = mlflow.pyfunc.load_model(model_save_path)\n",
    "predictions = classifier.predict(input_dict)\n",
    "print(json.dumps(predictions, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that the model has been correctly saved inside the `model` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLmodel          conda.yaml       python_model.pkl requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ./mlflow_custom_pyfunc_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Docker Hub credentials securely\n",
    "\n",
    "Now we prompt the user (you!) for your docker hub username and password in such a way that the value itself doesn't get written into the notebook, which is sensible security best-practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import base64\n",
    "username = getpass.getpass('docker hub username')\n",
    "password = getpass.getpass('docker hub password')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the data\n",
    "\n",
    "Now we can construct the metadata that the chassis service needs to build and publish the container to docker hub. In case `publish` is `False` the image is not uploaded to Docker Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = {\n",
    "    'name': f'{username}/chassisml-pytorch-ic:latest',\n",
    "    'version': '0.0.1',\n",
    "    'model_name': 'imagenet',\n",
    "    'model_path': './mlflow_custom_pyfunc_torch',\n",
    "    'registry_auth': base64.b64encode(f\"{username}:{password}\".encode(\"utf-8\")).decode(\"utf-8\"),\n",
    "    'publish': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Modzy data\n",
    "\n",
    "In the case of KFServing, no more data is required. When it comes to Modzy, we will need to define some more data:\n",
    "\n",
    "* `metadata_path`: this is the path to the [model.yaml](https://models.modzy.com/docs/model-packaging/model-packaging-python-template/yaml-file_) file that is needed to define all information about the model. Chassis has a default one, but you should define your own based on [this example](https://github.com/modzy/chassis/blob/main/chassisml-sdk/examples/modzy/model.yaml)\n",
    "* `sample_input_path`: this is the path to the [sample input](https://models.modzy.com/docs/model-deployment/model-deployment/input-outputs) that is needed when deploying the model. An example can be found [here](https://github.com/modzy/chassis/blob/main/chassisml-sdk/examples/modzy/input_sample.json)\n",
    "* `deploy`: if it is `True` Chassis will manage to deploy the model into Modzy platform. Otherwise you can do this manually through the Modzy UI\n",
    "* `api_key`: you should have your own [api key](https://models.modzy.com/docs/how-to-guides/api-keys) from Modzy in order to let Chassis deploy the model for you\n",
    "\n",
    "Notice that if `deploy` is False this means that you can avoid defining the rest of the fields. Anyway, `metadata_path` should be defined in case you will eventually deploy the model to Modzy. This is important because the model will use this information when being deployed to Modzy, so it needs to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "modzy_api_key = getpass.getpass('modzy api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "modzy_data = {\n",
    "    'metadata_path': './modzy/model-torch.yaml',\n",
    "    'sample_input_path': './modzy/airplane.jpg',\n",
    "    'deploy': True,\n",
    "    'api_key': modzy_api_key\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward ports to access service and registry\n",
    "\n",
    "This assumes that you are running these commands on your own terminal to redirect the service (port 5000) and the registry (port 5001) to localhost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "! # kubectl port-forward service/chassis 5000:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the job\n",
    "\n",
    "Important fields that we should fill in here are:\n",
    "\n",
    "* `module`: library that has been used to create the model\n",
    "* `image_data`: the values defined above\n",
    "* `image_type`: this is needed in case we are training images so afterwards the proxy will know how to interpret data\n",
    "* `base_url`: point to your running Chassis service here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building image... Ok!\n",
      "Job ID: chassis-builder-job-d9830bd0-91fd-4c6c-b669-0e6a73847ce8\n"
     ]
    }
   ],
   "source": [
    "# base_url must point to a running Chassis instance\n",
    "# in this example, we assume that Chassis is running locally\n",
    "res = chassisml.publish(\n",
    "    image_data=image_data,\n",
    "    modzy_data=modzy_data,\n",
    "    base_url='http://localhost:5000'\n",
    ")\n",
    "\n",
    "error = res.get('error')\n",
    "job_id = res.get('job_id')\n",
    "\n",
    "if error:\n",
    "    print('Error:', error)\n",
    "else:\n",
    "    print('Job ID:', job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the request is made, Chassis launches a job that runs Kaniko and builds the docker image based on the values provided.\n",
    "\n",
    "You can get the id of the job created from the result of the request. This id can be used to ask for the status of the job.\n",
    "\n",
    "This is an example of the data that is shown when the job has not finished yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': {'containerImage': {'containerImageSize': 0,\n",
       "   'loadPercentage': 10,\n",
       "   'loadStatus': 'IN_PROGRESS',\n",
       "   'repositoryName': 'vpf0yrzjle',\n",
       "   'uploadPercentage': 0,\n",
       "   'uploadStatus': 'IN_PROGRESS'},\n",
       "  'container_url': 'https://integration.modzy.engineering/models/vpf0yrzjle/0.1.0',\n",
       "  'createdAt': '2021-11-16T23:12:53.135+00:00',\n",
       "  'inputValidationSchema': '',\n",
       "  'inputs': [{'acceptedMediaTypes': 'image/jpeg',\n",
       "    'description': 'input RGB jpeg image',\n",
       "    'maximumSize': 5000000,\n",
       "    'name': 'input.jpg'}],\n",
       "  'isActive': False,\n",
       "  'isAvailable': True,\n",
       "  'longDescription': 'It classifies images.',\n",
       "  'model': {'author': 'Integration',\n",
       "   'createdByEmail': 'saumil.dave@modzy.com',\n",
       "   'description': 'This is an image built by chassis that exposes a PyTorch model.',\n",
       "   'features': [],\n",
       "   'isActive': False,\n",
       "   'isCommercial': False,\n",
       "   'isRecommended': False,\n",
       "   'latestActiveVersion': '',\n",
       "   'latestVersion': '0.1.0',\n",
       "   'modelId': 'vpf0yrzjle',\n",
       "   'name': 'chassis-torch-ic',\n",
       "   'permalink': 'vpf0yrzjle-integration-chassis-torch-ic',\n",
       "   'tags': [],\n",
       "   'versions': ['0.1.0'],\n",
       "   'visibility': {'scope': 'ALL'}},\n",
       "  'outputs': [{'description': 'class predictions',\n",
       "    'maximumSize': 1000000,\n",
       "    'mediaType': 'application/json',\n",
       "    'name': 'results.json'}],\n",
       "  'requirement': {'requirementId': -3},\n",
       "  'statistics': [],\n",
       "  'status': 'partial',\n",
       "  'technicalDetails': '#OVERVIEW:\\n\\n#TRAINING:\\n\\n#VALIDATION:\\n\\n#INPUT SPECIFICATION:\\nThe input(s) to this model must adhere to the following specifications:\\n| Filename      | Maximum Size | Accepted Format(s) |\\n| --------      | ------------ | ------------------ |\\n\\nAdditional information describing input file(s) can go in a short paragraph here if necessary. Feel free to add an additional markdown table if many values need to be listed.\\n\\n#OUTPUT DETAILS:\\nThis model will output the following:\\n| Filename      | Maximum Size | Format |\\n| --------      | ------------ | ------ |\\n\\nAdditional information describing the output file(s) can go in a short paragraph here. Feel free to add an additional markdown table if many values need to be listed. If you want to use an additional table, please use the following headerless format:\\n| | | | |\\n|-|-|-|-|\\n| Entry 1 | Entry 2 | Entry 3 | Entry 4 |\\n| Entry 5 | Entry 6 | Entry 7 | Entry 8 |',\n",
       "  'timeout': {'run': 60000, 'status': 60000},\n",
       "  'updatedAt': '2021-11-16T23:12:54.512+00:00',\n",
       "  'version': '0.1.0'},\n",
       " 'status': {'active': None,\n",
       "  'completion_time': 'Tue, 16 Nov 2021 23:15:01 GMT',\n",
       "  'conditions': [{'last_probe_time': 'Tue, 16 Nov 2021 23:15:01 GMT',\n",
       "    'last_transition_time': 'Tue, 16 Nov 2021 23:15:01 GMT',\n",
       "    'message': None,\n",
       "    'reason': None,\n",
       "    'status': 'True',\n",
       "    'type': 'Complete'}],\n",
       "  'failed': None,\n",
       "  'start_time': 'Tue, 16 Nov 2021 23:02:17 GMT',\n",
       "  'succeeded': 1}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chassisml.get_job_status(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is an example of the data that is shown when the job has already finished.\n",
    "\n",
    "Two top keys can be seen:\n",
    "\n",
    "* `result`: in case `deploy` was `True` this will contain some information related to the model deployed in Modzy. In particular we can see the full url to the model if we access the `container_url` key.\n",
    "* `status`: this contains the information about the kubernetes job that has built the image (and uploaded it to Modzy in case `deploy` was `True` as explained above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': {'containerImage': {'containerImageSize': 0,\n",
       "   'loadPercentage': 10,\n",
       "   'loadStatus': 'IN_PROGRESS',\n",
       "   'repositoryName': 'vpf0yrzjle',\n",
       "   'uploadPercentage': 0,\n",
       "   'uploadStatus': 'IN_PROGRESS'},\n",
       "  'container_url': 'https://integration.modzy.engineering/models/vpf0yrzjle/0.1.0',\n",
       "  'createdAt': '2021-11-16T23:12:53.135+00:00',\n",
       "  'inputValidationSchema': '',\n",
       "  'inputs': [{'acceptedMediaTypes': 'image/jpeg',\n",
       "    'description': 'input RGB jpeg image',\n",
       "    'maximumSize': 5000000,\n",
       "    'name': 'input.jpg'}],\n",
       "  'isActive': False,\n",
       "  'isAvailable': True,\n",
       "  'longDescription': 'It classifies images.',\n",
       "  'model': {'author': 'Integration',\n",
       "   'createdByEmail': 'saumil.dave@modzy.com',\n",
       "   'description': 'This is an image built by chassis that exposes a PyTorch model.',\n",
       "   'features': [],\n",
       "   'isActive': False,\n",
       "   'isCommercial': False,\n",
       "   'isRecommended': False,\n",
       "   'latestActiveVersion': '',\n",
       "   'latestVersion': '0.1.0',\n",
       "   'modelId': 'vpf0yrzjle',\n",
       "   'name': 'chassis-torch-ic',\n",
       "   'permalink': 'vpf0yrzjle-integration-chassis-torch-ic',\n",
       "   'tags': [],\n",
       "   'versions': ['0.1.0'],\n",
       "   'visibility': {'scope': 'ALL'}},\n",
       "  'outputs': [{'description': 'class predictions',\n",
       "    'maximumSize': 1000000,\n",
       "    'mediaType': 'application/json',\n",
       "    'name': 'results.json'}],\n",
       "  'requirement': {'requirementId': -3},\n",
       "  'statistics': [],\n",
       "  'status': 'partial',\n",
       "  'technicalDetails': '#OVERVIEW:\\n\\n#TRAINING:\\n\\n#VALIDATION:\\n\\n#INPUT SPECIFICATION:\\nThe input(s) to this model must adhere to the following specifications:\\n| Filename      | Maximum Size | Accepted Format(s) |\\n| --------      | ------------ | ------------------ |\\n\\nAdditional information describing input file(s) can go in a short paragraph here if necessary. Feel free to add an additional markdown table if many values need to be listed.\\n\\n#OUTPUT DETAILS:\\nThis model will output the following:\\n| Filename      | Maximum Size | Format |\\n| --------      | ------------ | ------ |\\n\\nAdditional information describing the output file(s) can go in a short paragraph here. Feel free to add an additional markdown table if many values need to be listed. If you want to use an additional table, please use the following headerless format:\\n| | | | |\\n|-|-|-|-|\\n| Entry 1 | Entry 2 | Entry 3 | Entry 4 |\\n| Entry 5 | Entry 6 | Entry 7 | Entry 8 |',\n",
       "  'timeout': {'run': 60000, 'status': 60000},\n",
       "  'updatedAt': '2021-11-16T23:12:54.512+00:00',\n",
       "  'version': '0.1.0'},\n",
       " 'status': {'active': None,\n",
       "  'completion_time': 'Tue, 16 Nov 2021 23:15:01 GMT',\n",
       "  'conditions': [{'last_probe_time': 'Tue, 16 Nov 2021 23:15:01 GMT',\n",
       "    'last_transition_time': 'Tue, 16 Nov 2021 23:15:01 GMT',\n",
       "    'message': None,\n",
       "    'reason': None,\n",
       "    'status': 'True',\n",
       "    'type': 'Complete'}],\n",
       "  'failed': None,\n",
       "  'start_time': 'Tue, 16 Nov 2021 23:02:17 GMT',\n",
       "  'succeeded': 1}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_status = chassisml.get_job_status(job_id)\n",
    "result = job_status.get('result')\n",
    "\n",
    "job_status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference!\n",
    "\n",
    "Now that the model has been deployed into Modzy platform, we can make a request against it to see it working. \n",
    "\n",
    "This is going to use the sample input defined above, which is going to be wrapped in a [Modzy Job](https://models.modzy.com/docs/jobs/jobs/submit-job-text). Take into account that input names must match model input filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'identifier': 'vpf0yrzjle',\n",
       "  'version': '0.1.0',\n",
       "  'name': 'chassis-torch-ic'},\n",
       " 'status': 'SUBMITTED',\n",
       " 'totalInputs': 1,\n",
       " 'jobIdentifier': '1c8f2aca-ca98-4311-b231-2737066aac1d',\n",
       " 'accessKey': 'q6ED7pBAFtCsDkbaBLpt',\n",
       " 'explain': False,\n",
       " 'jobType': 'batch',\n",
       " 'accountIdentifier': 'modzy-account',\n",
       " 'team': {'identifier': '830b2012-1557-48a9-a4df-d867b5a0939a'},\n",
       " 'user': {'identifier': 'eb8cf7ea-3930-410d-812f-7430e6baf3c5',\n",
       "  'externalIdentifier': 'saumil.dave@modzy.com',\n",
       "  'firstName': 'Saumil',\n",
       "  'lastName': 'Dave',\n",
       "  'email': 'saumil.dave@modzy.com',\n",
       "  'status': 'active'},\n",
       " 'jobInputs': {'identifier': ['input']},\n",
       " 'submittedAt': '2021-11-16T23:17:51.689+00:00',\n",
       " 'hoursDeleteInput': 24,\n",
       " 'imageClassificationModel': True}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "input_name = result['inputs'][0]['name'] # e.g. input.json\n",
    "input_b64 = base64.b64encode(open('./modzy/airplane.jpg','rb').read()).decode('ascii')\n",
    "\n",
    "request_data = {\n",
    "  'model': {\n",
    "    'identifier': f'{result.get(\"model\").get(\"modelId\")}',\n",
    "    'version': f'{result.get(\"version\")}'\n",
    "  },\n",
    "  'input': {\n",
    "    'type': 'embedded',\n",
    "    'sources': {\n",
    "      'input': {\n",
    "        input_name: f'data:image/jpeg;base64,{input_b64}'\n",
    "        # ^ this has to match the input filename specified when the model was created\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "import requests\n",
    "\n",
    "res_job = requests.post(\n",
    "    'https://integration.modzy.engineering/api/jobs',\n",
    "    json=request_data,\n",
    "    headers={'Authorization': f'ApiKey {modzy_api_key}'}\n",
    ")\n",
    "\n",
    "res_job_json = res_job.json()\n",
    "\n",
    "res_job_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must wait until the job has finished and we can see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobIdentifier': '1c8f2aca-ca98-4311-b231-2737066aac1d',\n",
       " 'accountIdentifier': 'modzy-account',\n",
       " 'team': {'identifier': '830b2012-1557-48a9-a4df-d867b5a0939a'},\n",
       " 'total': 1,\n",
       " 'completed': 1,\n",
       " 'failed': 0,\n",
       " 'finished': True,\n",
       " 'submittedByKey': 'q6ED7pBAFtCsDkbaBLpt',\n",
       " 'explained': False,\n",
       " 'submittedAt': '2021-11-16T23:17:51.689+00:00',\n",
       " 'initialQueueTime': 45054,\n",
       " 'totalQueueTime': 45054,\n",
       " 'averageModelLatency': 2814.0,\n",
       " 'totalModelLatency': 2814.0,\n",
       " 'elapsedTime': 49311,\n",
       " 'startingResultSummarizing': '2021-11-16T23:18:40.394+00:00',\n",
       " 'resultSummarizing': 606,\n",
       " 'inputSize': 383,\n",
       " 'results': {'input': {'status': 'SUCCESSFUL',\n",
       "   'engine': 'model-batch-vpf0yrzjle-0-1-0-859664b44f-flbwn',\n",
       "   'inputFetching': 2611,\n",
       "   'outputUploading': None,\n",
       "   'modelLatency': 2814.0,\n",
       "   'queueTime': 45054,\n",
       "   'startTime': '2021-11-16T23:18:36.539+0000',\n",
       "   'updateTime': '2021-11-16T23:18:40.298+0000',\n",
       "   'endTime': '2021-11-16T23:18:40.298+0000',\n",
       "   'results.json': {'data': {'result': {'classPredictions': [{'class': 'airliner',\n",
       "        'score': 0.9193505644798279},\n",
       "       {'class': 'wing', 'score': 0.054149091243743896},\n",
       "       {'class': 'warplane, military plane', 'score': 0.010522918775677681},\n",
       "       {'class': 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
       "        'score': 0.004400264471769333},\n",
       "       {'class': 'crane', 'score': 0.0019400734454393387}]},\n",
       "     'explanation': None,\n",
       "     'drift': None}},\n",
       "   'voting': {'up': 0, 'down': 0}}}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_id = res_job_json.get('jobIdentifier')\n",
    "\n",
    "res_result = requests.get(\n",
    "    f'https://integration.modzy.engineering/api/results/{job_id}',\n",
    "    headers={'Authorization': f'ApiKey {modzy_api_key}'}\n",
    ")\n",
    "\n",
    "res_result_json = res_result.json()\n",
    "\n",
    "res_result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
